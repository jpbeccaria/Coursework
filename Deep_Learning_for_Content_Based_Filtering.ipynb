{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwF+tiqLWB4q4PVbaeIprp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpbeccaria/Playroom/blob/main/Deep_Learning_for_Content_Based_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfX-Po_32rNK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tabulate\n",
        "from recsysNN_utils import *\n",
        "pd.set_option(\"display.precision\", 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top10_df = pd.read_csv(\"./data/content_top10_df.csv\")\n",
        "bygenre_df = pd.read_csv(\"./data/content_bygenre_df.csv\")\n",
        "top10_df"
      ],
      "metadata": {
        "id": "SNQgJ7ll2sST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bygenre_df"
      ],
      "metadata": {
        "id": "li-olWtr2sVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data, set configuration variables\n",
        "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
        "\n",
        "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
        "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
        "uvs = 3  # user genre vector start\n",
        "ivs = 3  # item genre vector start\n",
        "u_s = 3  # start of columns to use in training, user\n",
        "i_s = 1  # start of columns to use in training, items\n",
        "print(f\"Number of training vectors: {len(item_train)}\")"
      ],
      "metadata": {
        "id": "4d3dK37h2sX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint_train(user_train, user_features, uvs,  u_s, maxcount=5)"
      ],
      "metadata": {
        "id": "pb6KOpZF2sag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)"
      ],
      "metadata": {
        "id": "jOjr-irH2sdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"y_train[:5]: {y_train[:5]}\")"
      ],
      "metadata": {
        "id": "FGvNUL0t2sfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale training data\n",
        "item_train_unscaled = item_train\n",
        "user_train_unscaled = user_train\n",
        "y_train_unscaled    = y_train\n",
        "\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
        "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))"
      ],
      "metadata": {
        "id": "fZuiUGf82siS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
        "print(f\"movie/item training data shape: {item_train.shape}\")\n",
        "print(f\"movie/item test data shape: {item_test.shape}\")"
      ],
      "metadata": {
        "id": "SkCqhlwT2sk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
      ],
      "metadata": {
        "id": "iOQhuCKn2snf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Neural Network for content-based filtering"
      ],
      "metadata": {
        "id": "-jJbF6Zk2sqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED_CELL\n",
        "# UNQ_C1\n",
        "\n",
        "num_outputs = 32\n",
        "tf.random.set_seed(1)\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation ='relu'),\n",
        "    tf.keras.layers.Dense(128, activation ='relu'),\n",
        "    tf.keras.layers.Dense(32)\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation ='relu'),\n",
        "    tf.keras.layers.Dense(128, activation ='relu'),\n",
        "    tf.keras.layers.Dense(32)  \n",
        "])\n",
        "\n",
        "# create the user input and point to the base network\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "# create the item input and point to the base network\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "# compute the dot product of the two vectors vu and vm\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "# specify the inputs and output of the model\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hF6opBLL2stO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Public tests\n",
        "from public_tests import *\n",
        "test_tower(user_NN)\n",
        "test_tower(item_NN)"
      ],
      "metadata": {
        "id": "6TTY1KDe2sv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=cost_fn)"
      ],
      "metadata": {
        "id": "TprC02nV2syu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)"
      ],
      "metadata": {
        "id": "UaV6g7-03DV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
      ],
      "metadata": {
        "id": "9BcrDWw-3DYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_user_id = 5000\n",
        "new_rating_ave = 0.0\n",
        "new_action = 0.0\n",
        "new_adventure = 5.0\n",
        "new_animation = 0.0\n",
        "new_childrens = 0.0\n",
        "new_comedy = 0.0\n",
        "new_crime = 0.0\n",
        "new_documentary = 0.0\n",
        "new_drama = 0.0\n",
        "new_fantasy = 5.0\n",
        "new_horror = 0.0\n",
        "new_mystery = 0.0\n",
        "new_romance = 0.0\n",
        "new_scifi = 0.0\n",
        "new_thriller = 0.0\n",
        "new_rating_count = 3\n",
        "\n",
        "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
        "                      new_action, new_adventure, new_animation, new_childrens,\n",
        "                      new_comedy, new_crime, new_documentary,\n",
        "                      new_drama, new_fantasy, new_horror, new_mystery,\n",
        "                      new_romance, new_scifi, new_thriller]])"
      ],
      "metadata": {
        "id": "BTj7NsoJ3Da0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and replicate the user vector to match the number movies in the data set.\n",
        "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "\n",
        "print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)"
      ],
      "metadata": {
        "id": "b1wp0HJN3DdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uid = 2 \n",
        "# form a set of user vectors. This is the same vector, transformed and repeated.\n",
        "user_vecs, y_vecs = get_user_vecs(uid, user_train_unscaled, item_vecs, user_to_genre)\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "sorted_user  = user_vecs[sorted_index]\n",
        "sorted_y     = y_vecs[sorted_index]\n",
        "\n",
        "#print sorted predictions for movies rated by the user\n",
        "print_existing_user(sorted_ypu, sorted_y.reshape(-1,1), sorted_user, sorted_items, ivs, uvs, movie_dict, maxcount = 50)"
      ],
      "metadata": {
        "id": "B9x4T6si3DgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sq_dist(a,b):\n",
        "    \"\"\"\n",
        "    Returns the squared distance between two vectors\n",
        "    Args:\n",
        "      a (ndarray (n,)): vector with n features\n",
        "      b (ndarray (n,)): vector with n features\n",
        "    Returns:\n",
        "      d (float) : distance\n",
        "    \"\"\"    \n",
        "    n = a.shape[0]\n",
        "    d = 0\n",
        "\n",
        "    for l in range(n):\n",
        "        d += (a[l]-b[l])**2\n",
        "  \n",
        "    return d"
      ],
      "metadata": {
        "id": "suk1FylG3Di8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = np.array([1.0, 2.0, 3.0]); b1 = np.array([1.0, 2.0, 3.0])\n",
        "a2 = np.array([1.1, 2.1, 3.1]); b2 = np.array([1.0, 2.0, 3.0])\n",
        "a3 = np.array([0, 1, 0]);       b3 = np.array([1, 0, 0])\n",
        "print(f\"squared distance between a1 and b1: {sq_dist(a1, b1):0.3f}\")\n",
        "print(f\"squared distance between a2 and b2: {sq_dist(a2, b2):0.3f}\")\n",
        "print(f\"squared distance between a3 and b3: {sq_dist(a3, b3):0.3f}\")"
      ],
      "metadata": {
        "id": "XOB-0iAH5PO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_item_m = tf.keras.layers.Input(shape=(num_item_features))    # input layer\n",
        "vm_m = item_NN(input_item_m)                                       # use the trained item_NN\n",
        "vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        # incorporate normalization as was done in the original model\n",
        "model_m = tf.keras.Model(input_item_m, vm_m)                                \n",
        "model_m.summary()"
      ],
      "metadata": {
        "id": "hnbYEA175PRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_item_vecs = scalerItem.transform(item_vecs)\n",
        "vms = model_m.predict(scaled_item_vecs[:,i_s:])\n",
        "print(f\"size of all predicted movie feature vectors: {vms.shape}\")"
      ],
      "metadata": {
        "id": "WA610BDE5PUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 50  # number of movies to display\n",
        "dim = len(vms)\n",
        "dist = np.zeros((dim,dim))\n",
        "\n",
        "for i in range(dim):\n",
        "    for j in range(dim):\n",
        "        dist[i,j] = sq_dist(vms[i, :], vms[j, :])\n",
        "        \n",
        "m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal\n",
        "\n",
        "disp = [[\"movie1\", \"genres\", \"movie2\", \"genres\"]]\n",
        "for i in range(count):\n",
        "    min_idx = np.argmin(m_dist[i])\n",
        "    movie1_id = int(item_vecs[i,0])\n",
        "    movie2_id = int(item_vecs[min_idx,0])\n",
        "    disp.append( [movie_dict[movie1_id]['title'], movie_dict[movie1_id]['genres'],\n",
        "                  movie_dict[movie2_id]['title'], movie_dict[movie1_id]['genres']]\n",
        "               )\n",
        "table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
        "table"
      ],
      "metadata": {
        "id": "28-HnZT15PWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "na8DTu705PZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}